{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rze7aK4lBqM1"
      },
      "source": [
        "# **Week 4: Lab Exercises for COMP499/691 Conversational AI**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10INNHzQBvlF"
      },
      "source": [
        "This project aims to create a task-oriented, multi-turn dialogue system using the popular Ubuntu Dialogue Corpus. The corpus contains dialogues from an ubuntu support chat.\n",
        "\n",
        "## **Task Description**\n",
        "\n",
        "Next Utterance Classification (NUC) is a task whose objective is to have the model attempt to distinguish between valid and invalid responses based on the context of a given conversation. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ircn1u15FWqd"
      },
      "source": [
        "**Run the code below** to download the Ubuntu Dialogue Corpus data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2ueEnamV8j8",
        "outputId": "b6ebe517-b016-4288-9625-613ed590acd5"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "from nltk import download, word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "import pandas as pd\n",
        "\n",
        "download('punkt', quiet=True)\n",
        "\n",
        "DATA_FOLDER='./src'\n",
        "TRAIN_FILE='train_large.csv'\n",
        "VALID_FILE='valid.csv'\n",
        "TEST_FILE='test.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q55C-YBYAE6M",
        "outputId": "8878a967-1f2c-4602-c117-47c985855b4a"
      },
      "outputs": [],
      "source": [
        "# Update gdown for the latest version\n",
        "!pip install --upgrade --no-cache-dir gdown\n",
        "\n",
        "!gdown 1UNaMsyhqzYYe5jOQH0p7QI99_WTil2rW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlrZk7fQvy_N"
      },
      "source": [
        "Once downloaded, we can follow it up by uncompress the data. **Run the code below** to unzip the downloaded file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqTQRerkvoy2"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!unzip UDC.zip -d raw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijlWzY1mIhCF"
      },
      "source": [
        "The dataset is split into training, validation, and test data as you can see by opening `train.csv`, `valid.csv`, and `test.csv`.\n",
        "\n",
        "The data is formatted differently for the training data than it is for the validation and test data. \n",
        "\n",
        "The train data is separated into 3 columns: the context of the conversation (`Context`), the candidate response (`Utterance`), and a flag (`Label`) denoting whether the response is a 'true response' to the context (flag = 1), or a randomly drawn response from elsewhere in the dataset (flag = 0).\n",
        "\n",
        "The validation and test data contain 11 columns. The row represents a question. Separated into 11 columns: the context (`Context`), the true response to the context (`Ground Truth Utterance`), and 9 false responses (`Distractors`) that were randomly sampled from elsewhere in the dataset. The model gets a question correct if it selects the ground truth utterance from amongst the 10 possible responses.\n",
        "\n",
        "There are 1 000 000 sentences for training, 19 560 for validation, and 18 920 for testing."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wfRpWdvS5iwc"
      },
      "source": [
        "## **Step 0. Creating Word Embeddings**\n",
        "\n",
        "**Run the code below** to create the word embeddings needed for training our models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "37apT3q56Ip0",
        "outputId": "eea52f41-7740-4a65-dde9-84b2ef07191b"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('./raw/%s' % TRAIN_FILE)\n",
        "train['Combined'] = train['Context'] + ' ' + train['Utterance']\n",
        "path = './src/train_large_preprocessed.csv'\n",
        "train.to_csv(path, columns=['Combined'], sep='Å±', escapechar='`', header=False, index=False, quoting=csv.QUOTE_NONE)\n",
        "\n",
        "# This is attempt number 2. It requires a lot of RAM to run, and ~5 minutes to fully complete\n",
        "# train['Tokenized'] = train.apply(lambda row: word_tokenize(row['Context'] + ' ' + row['Utterance']), axis=1)\n",
        "# train['Tokenized'].memory_usage()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imCLdJj650iL",
        "outputId": "fde542f8-d41c-4e85-e25e-f5bb1d245eb6"
      },
      "outputs": [],
      "source": [
        "# Parameters to train the word2vec model\n",
        "embedding_size = 300\n",
        "window_size=7\n",
        "min_count=1\n",
        "workers=3\n",
        "\n",
        "# Word2Vec Model Training. Takes around ~6 mins to complete\n",
        "model = Word2Vec(\n",
        "  vector_size=embedding_size,\n",
        "  window=window_size,\n",
        "  min_count=min_count,\n",
        "  workers=workers\n",
        ")\n",
        "print('Building vocabulary...')\n",
        "model.build_vocab(corpus_file='./src/train_preprocessed.csv', keep_raw_vocab=False)\n",
        "\n",
        "print('Training the model...')\n",
        "model.train(corpus_file='./src/train_preprocessed.csv', total_words=90959873, total_examples=1000000, epochs=model.epochs)\n",
        "\n",
        "print('Done!')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Run the code below** to save the word2vec model as a file for later use by Speechbrain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The model will save the following files: word2vec.model, word2vec.model.syn1neg.npy and word2vec.model.wv.vectors.npy\n",
        "model.save('./src/models/word2vec.model')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 1: Preparing the data\n",
        "\n",
        "Speechbrain expects the data in a particular standard, so it is important for us to make sure that we can give it as such. we will rework our train file in order for it to be accepted by Speechbrain's Dynamic Item Processing pipeline.\n",
        "\n",
        "**Run the code below** to load the dataframe to the train file, and add a numbered ID column to the whole dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Data Cleaning\n",
        "def generate_id(s):\n",
        "    return abs(hash(s)) % (10 ** 10)\n",
        "\n",
        "\n",
        "# - Remove variables (ex: $DEITY, $PLAN)\n",
        "for dataset in [TRAIN_FILE, VALID_FILE, TEST_FILE]:\n",
        "\n",
        "  df = pd.read_csv('%s/%s' % (DATA_FOLDER, dataset)) \n",
        "  # Remove $ characters\n",
        "  df.replace('\\$','',inplace=True, regex=True)\n",
        "  df['ID'] = 'utt' + df.index.map(str)\n",
        "  df.set_index('ID', inplace=True)\n",
        "  df.to_csv('%s/data/%s' % (DATA_FOLDER, dataset), index_label='ID')\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LzxRMaTIJWQd"
      },
      "source": [
        "## **Approach 1: LSTM-based encoder-decoder architecture with attention**\n",
        "\n",
        "We will start with a simple RNN-based encoder-decoder architecture, that only considers the response without looking at the dialogue context.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa2y3zXDKpEi"
      },
      "source": [
        "**Run the code below** to install speechbrain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQe4BzlIBfpm"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install speechbrain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySUW0E91KwZY"
      },
      "source": [
        "You wrote the following yaml file that declares all the components that we need.\n",
        "\n",
        "**Run the code below** to save the hyperparameter file. Please, revise it carefully to start familiarizing yourself with this format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suNuN7R6_sug",
        "outputId": "43da5adc-ac39-432d-95a9-377bc3e24747"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting LSTM_Single_Encoder.yaml\n"
          ]
        }
      ],
      "source": [
        "%%file LSTM_Single_Encoder.yaml\n",
        "\n",
        "# ############################################################################\n",
        "# Model: E2E with attention-based Machine Translation\n",
        "# Encoder: LSTM\n",
        "# Decoder: LSTM\n",
        "# Tokens: Words\n",
        "# losses: BCE\n",
        "# Training: UDC\n",
        "##############################################################################\n",
        "\n",
        "# Seed needs to be set at top of yaml, before objects with parameters are instantiated\n",
        "seed: 1337\n",
        "__set_seed: !apply:torch.manual_seed [!ref <seed>]\n",
        "# import gensim\n",
        "# model = gensim.models.KeyedVectors.load_word2vec_format('path/to/file')\n",
        "# weights = torch.FloatTensor(model.vectors)\n",
        "# Folder set up\n",
        "\n",
        "data_folder: !PLACEHOLDER\n",
        "\n",
        "\n",
        "output_folder: !ref results/LSTM_Single_Encoder/<seed>\n",
        "wer_file: !ref <output_folder>/wer.txt # NOTE: might not need this, word error rate\n",
        "save_folder: !ref <output_folder>/save\n",
        "train_log: !ref <output_folder>/train_log.txt\n",
        "cer_file: !ref <output_folder>/test_cer.txt # file were to store the final character error rate on the test set. NOTE: might not need that\n",
        "\n",
        "# Path where data manifest files are stored\n",
        "train_annotation: !ref <data_folder>/data/train.csv\n",
        "valid_annotation: !ref <data_folder>/data/valid.csv\n",
        "test_annotation: !ref <data_folder>/data/test.csv\n",
        "\n",
        "# Path where the embeddings are stored\n",
        "word2vec_file: !ref <data_folder>/models/w2v_raw/word2vec.model\n",
        "embeddings_file: !ref <data_folder>/models/w2v_raw/word2vec.model.wv.vectors.npy\n",
        "\n",
        "# The train logger writes training statistics to a file, as well as stdout.\n",
        "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
        "    save_file: !ref <train_log>\n",
        "\n",
        "# Training parameters\n",
        "# NOTE: Hyperparams taken from https://github.com/rkadlec/ubuntu-ranking-dataset-creator\n",
        "number_of_epochs: 100\n",
        "batch_size: 256\n",
        "lr: 0.001\n",
        "lr_final: 0.00005\n",
        "\n",
        "\n",
        "# Dataloader options\n",
        "train_dataloader_opts:\n",
        "    batch_size: !ref <batch_size>\n",
        "\n",
        "valid_dataloader_opts:\n",
        "    batch_size: !ref <batch_size>\n",
        "\n",
        "test_dataloader_opts:\n",
        "    batch_size: !ref <batch_size>\n",
        "\n",
        "\n",
        "# Vocabulary size\n",
        "voc_size: 1344621 # crazy TODO: May need it if we want to finetune embedding encoder\n",
        "\n",
        "# Number of flags (good or bad response)\n",
        "n_classes: 2\n",
        "\n",
        "# Indexes for begin-of-sentence (bos) \n",
        "# and end-of-sentence (eos) # TODO not needed either perhaps\n",
        "blank_index: 0 # This special token is for padding\n",
        "bos_index: 1\n",
        "eos_index: 2\n",
        "\n",
        "\n",
        "# Encoder Parameters\n",
        "enc_hidden_size: 300\n",
        "enc_num_layers: 2 # Not sure about that, start with 1 for now\n",
        "\n",
        "# Decoder Parameters\n",
        "dec_hidden_size: 300 # Not sure about that either\n",
        "dec_num_layers: 3   \n",
        "attn_dim: 300\n",
        "dropout: 0.5\n",
        "\n",
        "# Epoch Counter\n",
        "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
        "    limit: !ref <number_of_epochs>\n",
        "\n",
        "# Response Encoder (Good)\n",
        "response_encoder: !new:speechbrain.nnet.RNN.LSTM\n",
        "    input_size: !ref <enc_hidden_size>\n",
        "    hidden_size: !ref <enc_hidden_size>\n",
        "    num_layers: !ref <enc_num_layers>\n",
        "    dropout: !ref <dropout>\n",
        "\n",
        "# NOTE: For the next step we may need to create a second encoder\n",
        "#       To encode the context as part of the design\n",
        "\n",
        "# Embeddings \n",
        "# Load the word2vec model\n",
        "w2v_model: !apply:gensim.models.Word2Vec.load [!ref <word2vec_file>]\n",
        "\n",
        "# Load the numpy file created by word2vec\n",
        "vectors: !apply:numpy.load [!ref <embeddings_file>]\n",
        "\n",
        "weights: !new:torch.FloatTensor\n",
        "    data: !ref <vectors>\n",
        "    \n",
        "input_embeddings: !apply:torch.nn.Embedding.from_pretrained [!ref <weights>]\n",
        "    \n",
        "# Attention-based RNN decoder - (Okay, this also seems weird)\n",
        "decoder: !new:speechbrain.nnet.RNN.AttentionalRNNDecoder\n",
        "    enc_dim: !ref <enc_hidden_size>\n",
        "    input_size: !ref <dec_hidden_size>\n",
        "    rnn_type: lstm\n",
        "    attn_type: content\n",
        "    hidden_size: !ref <dec_hidden_size>\n",
        "    attn_dim: !ref <attn_dim>\n",
        "    num_layers: !ref <dec_num_layers>\n",
        "    dropout: !ref <dropout>\n",
        "\n",
        "# Embeddings - (Is this needed? Once again, we're not giving values in)\n",
        "decoder_emb: !new:torch.nn.Embedding\n",
        "    num_embeddings: !ref <voc_size>\n",
        "    embedding_dim: !ref <dec_hidden_size>\n",
        "    padding_idx: !ref <blank_index>\n",
        "\n",
        "# Linear transformation on the top of the last hidden layer (This makes sense)\n",
        "# classifier: !new:torch.nn.Sigmoid\n",
        "\n",
        "classifier: !new:speechbrain.nnet.linear.Linear\n",
        "    input_size: !ref <dec_hidden_size>\n",
        "    n_neurons: !ref <n_classes>\n",
        "\n",
        "# # Final softmax (for log posteriors computation).\n",
        "# log_softmax: !new:speechbrain.nnet.activations.Softmax\n",
        "#     apply_log: True\n",
        "\n",
        "\n",
        "# Objects in \"modules\" dict will have their parameters moved to the correct\n",
        "# device, as well as having train()/eval() called on them by the Brain class\n",
        "modules:\n",
        "    response_encoder: !ref <response_encoder>\n",
        "    classifier: !ref <classifier>\n",
        "    input_embeddings: !ref <input_embeddings>\n",
        "    decoder: !ref <decoder>\n",
        "    decoder_emb: !ref <response_encoder>\n",
        "    \n",
        "\n",
        "\n",
        "# Gathering all the submodels in a single model object.\n",
        "model: !new:torch.nn.ModuleList\n",
        "    - - !ref <response_encoder>\n",
        "      - !ref <classifier> \n",
        "      - !ref <input_embeddings>\n",
        "      - !ref <decoder>\n",
        "      - !ref <decoder_emb>\n",
        "     \n",
        "\n",
        "# This function manages learning rate annealing over the epochs.\n",
        "lr_annealing: !new:speechbrain.nnet.schedulers.LinearScheduler\n",
        "    initial_value: !ref <lr>\n",
        "    final_value: !ref <lr_final>\n",
        "    epoch_count: !ref <number_of_epochs>\n",
        "\n",
        "# This optimizer will be constructed by the Brain class after all parameters\n",
        "# are moved to the correct device. Then it will be added to the checkpointer.\n",
        "opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr>\n",
        "   \n",
        "# This object is used for saving the state of training both so that it\n",
        "# can be resumed if it gets interrupted, and also so that the best checkpoint\n",
        "# can be later loaded for evaluation or inference.\n",
        "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
        "    checkpoints_dir: !ref <save_folder>\n",
        "    recoverables:\n",
        "        model: !ref <model>\n",
        "        counter: !ref <epoch_counter>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jSvFt6KLQsp"
      },
      "source": [
        "The RNN.yaml hyperparameter file is coupled with the following training script (train.py).\n",
        "\n",
        "**Complete the code below** to implement the targeted system. You have to write your code in the `forward` and `compute_objective` methods where required. The code should match what is declared in the YAML file. Take this opportunity to further familiarize yourself with the SpeechBrain training script (they are all similar). In particular, try to understand the data flow: take a look at how we create and process entries declared in the CSV files, how we turn the sequence of chars into a sequence of corresponding integers,  how we turn the processed entries into batches, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_hiwAJ2AmO7",
        "outputId": "d401a173-9bf5-48b0-dceb-e5ec9780d8bc"
      },
      "outputs": [],
      "source": [
        "%%file train.py\n",
        "\n",
        "#!/usr/bin/env/python3\n",
        "\"\"\"Recipe for training a sequence-to-sequence machine translation system \n",
        "on \"ignotush\".\n",
        "The system employs an encoder, a decoder, and an attention mechanism\n",
        "between them. \n",
        "\n",
        "To run this recipe, do the following:\n",
        "> python train.py train.yaml\n",
        "\n",
        "With the default hyperparameters, the system employs an GRU encoder and decoder.\n",
        "\n",
        "The neural network is trained with the negative-log likelihood objective and\n",
        "characters are used as basic tokens for both english and ignotush.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import logging\n",
        "import speechbrain as sb\n",
        "from nltk import word_tokenize\n",
        "from hyperpyyaml import load_hyperpyyaml\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "# Brain class for speech recognition training\n",
        "class UbuntuBrain(sb.Brain):\n",
        "    \"\"\"Class that manages the training loop. See speechbrain.core.Brain.\"\"\"\n",
        "\n",
        "    def compute_forward(self, batch, stage):\n",
        "        \"\"\"Runs all the computation of the CTC + seq2seq ASR. It returns the\n",
        "        posterior probabilities of the CTC and seq2seq networks.\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        batch : PaddedBatch\n",
        "            This batch object contains all the relevant tensors for computation.\n",
        "        stage : sb.Stage\n",
        "            One of sb.Stage.TRAIN, sb.Stage.VALID, or sb.Stage.TEST.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        predictions : torch.tensor\n",
        "            Log-probabilities predicted by the decoder.\n",
        "            \n",
        "        At validation/test time, it returns the predicted tokens as well.\n",
        "        \"\"\"\n",
        "        \n",
        "        # We first move the batch to the appropriate device.\n",
        "        batch = batch.to(self.device) # todo\n",
        "\n",
        "        if stage == sb.Stage.TRAIN:\n",
        "            # Unpack inputs\n",
        "            utterances, lens = batch.utterance\n",
        "            # Input embeddings\n",
        "            embeddings = self.modules.input_embeddings(utterances)\n",
        "            # Running the encoder\n",
        "            encoded_signal, _ = self.modules.response_encoder(embeddings)\n",
        "            # Compute probability using a sigmoid\n",
        "            predictions = self.modules.classifier(encoded_signal[:,-1,:])\n",
        "            # Compute softmax\n",
        "            # predictions = self.hparams.log_softmax(logits)\n",
        "        elif stage == sb.Stage.VALID:\n",
        "            # Unpack the context\n",
        "            context, _ = batch.context \n",
        "            context_embeddings = self.modules.input_embeddings(context)\n",
        "            context_signal, _ = self.modules.response_encoder(context_embeddings)\n",
        "\n",
        "            probas = []\n",
        "            for response in [batch.ground_truth_utterance, batch.distractor_0]:\n",
        "                utt, _ = response\n",
        "                embeddings = self.modules.input_embeddings(utt)\n",
        "                encoded_signal, _ = self.modules.response_encoder(embeddings)\n",
        "                probas.append(self.modules.classifier(encoded_signal[:,-1,:]))\n",
        "            \n",
        "            # Unpack the responses\n",
        "            print([p for p in probas])\n",
        "            \n",
        "            # Getting some predictions.\n",
        "            # Note: For simplicity we use teacher forcing also for testing.\n",
        "            # For a real inference, you would need to implement greedy search \n",
        "            # and feed into the autoregressive model the actual predictions\n",
        "            # done at each step by the model. \n",
        "            hyps = predictions.argmax(-1)\n",
        "            \n",
        "            # getting the first index where the prediciton is eos_index\n",
        "            stop_indexes = (hyps==self.hparams.eos_index).int()\n",
        "            stop_indexes = stop_indexes.argmax(dim=1)\n",
        "            \n",
        "            # Converting hyps from indexes to chars\n",
        "            hyp_lst = []\n",
        "            for hyp, stop_ind in zip(hyps, stop_indexes):\n",
        "                # in some cases the eos in not observed (e.g, for the last sentence\n",
        "                # in the batch)\n",
        "                if stop_ind == 0:\n",
        "                    stop_ind = -1\n",
        "                # Stopping when eos is observed\n",
        "                hyp = hyp[0:stop_ind]\n",
        "                # From index to character\n",
        "                hyp_lst.append(self.label_encoder.decode_ndim(hyp))\n",
        "\n",
        "            return predictions, hyp_lst\n",
        "        \n",
        "        return predictions\n",
        "        \n",
        "\n",
        "    def compute_objectives(self, predictions, batch, stage):\n",
        "        \"\"\"Computes the loss given the predicted and targeted outputs.\n",
        "        \n",
        "        Arguments\n",
        "        ---------\n",
        "        predictions : torch.tTensor\n",
        "            The output tensor from `compute_forward`.\n",
        "        batch : PaddedBatch\n",
        "            This batch object contains all the relevant tensors for computation.\n",
        "        stage : sb.Stage\n",
        "            One of sb.Stage.TRAIN, sb.Stage.VALID, or sb.Stage.TEST.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        loss : torch.Tensor\n",
        "            A one-element tensor used for backpropagating the gradient.\n",
        "        \"\"\"\n",
        "\n",
        "        # Unpacking Labels\n",
        "        targets, _ = batch.label\n",
        "        \n",
        "        \n",
        "        # Reading the predictions\n",
        "        if stage == sb.Stage.TEST:\n",
        "          predictions, hyp_lst = predictions\n",
        "          \n",
        "          for id, inp, label, hyp in zip(batch.id, batch.ignotush_chars, batch.english_chars, hyp_lst):\n",
        "              print(id)\n",
        "              print(\"INP: \" + ''.join(inp))\n",
        "              print(\"REF: \" + ''.join(label))\n",
        "              print(\"HYP: \" + ''.join(hyp))\n",
        "              print('--------')\n",
        "        \n",
        "        # Computing the bce_loss\n",
        "        loss = sb.nnet.losses.bce_loss(predictions, targets)   \n",
        "        \n",
        "        return loss\n",
        "\n",
        "            \n",
        "    def on_stage_end(self, stage, stage_loss, epoch):\n",
        "        print(\"STAGE_END\")\n",
        "        \"\"\"Gets called at the end of an epoch.\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        stage : sb.Stage\n",
        "            One of sb.Stage.TRAIN, sb.Stage.VALID, sb.Stage.TEST\n",
        "        stage_loss : float\n",
        "            The average loss for all of the data processed in this stage.\n",
        "        epoch : int\n",
        "            The currently-starting epoch. This is passed\n",
        "            `None` during the test stage.\n",
        "        \"\"\"\n",
        "\n",
        "        # Store the train loss until the validation stage.\n",
        "        stage_stats = {\"loss\": stage_loss}\n",
        "        if stage == sb.Stage.TRAIN:\n",
        "            self.train_stats = stage_stats\n",
        "\n",
        "\n",
        "        # Perform end-of-iteration things, like annealing, logging, etc.\n",
        "        elif stage == sb.Stage.VALID:\n",
        "            # Update learning rate\n",
        "            old_lr, new_lr = self.hparams.lr_annealing(epoch)\n",
        "            sb.nnet.schedulers.update_learning_rate(self.optimizer, new_lr)\n",
        "\n",
        "            # The train_logger writes a summary to stdout and to the logfile.\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                stats_meta={\"epoch\": epoch, \"lr\": old_lr},\n",
        "                train_stats=self.train_stats,\n",
        "                valid_stats={\n",
        "                    \"loss\": stage_loss,\n",
        "                },\n",
        "            )\n",
        "            # Save the current checkpoint and delete previous checkpoints.\n",
        "            self.checkpointer.save_and_keep_only(\n",
        "                meta={\"loss\": stage_stats[\"loss\"]}, min_keys=[\"loss\"],\n",
        "            )\n",
        "\n",
        "        # We also write statistics about test data to stdout and to the logfile.\n",
        "        elif stage == sb.Stage.TEST:\n",
        "            \n",
        "            self.hparams.train_logger.log_stats(\n",
        "                stats_meta={\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
        "                test_stats={\n",
        "                    \"loss\": stage_loss,\n",
        "                },\n",
        "            )\n",
        "            \n",
        "\n",
        "def dataio_prepare(hparams):\n",
        "    \"\"\"This function prepares the datasets to be used in the brain class.\n",
        "    It also defines the data processing pipeline through user-defined functions.\n",
        "\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    hparams : dict\n",
        "        This dictionary is loaded from the `train.yaml` file, and it includes\n",
        "        all the hyperparameters needed for dataset construction and loading.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    datasets : dict\n",
        "        Dictionary containing \"train\", \"valid\", and \"test\" keys that correspond\n",
        "        to the DynamicItemDataset objects.\n",
        "    \"\"\"\n",
        "\n",
        "    def preprocess_text(text):\n",
        "        return [hparams['w2v_model'].wv.get_index(t, 0) for t in word_tokenize(text)]\n",
        "\n",
        "    @sb.utils.data_pipeline.takes(\"Context\", \"Utterance\", \"Label\")\n",
        "    @sb.utils.data_pipeline.provides(\"context\", \"utterance\", \"label\")\n",
        "    def train_text_pipeline(context, utterance, label):\n",
        "        \"\"\"Processes the transcriptions to generate proper labels\"\"\"\n",
        "        context = torch.LongTensor(preprocess_text(context))\n",
        "        yield context\n",
        "        utterance = torch.LongTensor(preprocess_text(utterance))\n",
        "        yield utterance\n",
        "        label = torch.LongTensor([float(label)])\n",
        "        yield label\n",
        "\n",
        "\n",
        "    @sb.utils.data_pipeline.takes(\"Context\",\"Ground Truth Utterance\",\"Distractor_0\",\"Distractor_1\",\"Distractor_2\",\"Distractor_3\",\"Distractor_4\",\"Distractor_5\",\"Distractor_6\",\"Distractor_7\",\"Distractor_8\")\n",
        "    @sb.utils.data_pipeline.provides(\"context\",\"ground_truth_utterance\",\"distractor_0\",\"distractor_1\",\"distractor_2\",\"distractor_3\",\"distractor_4\",\"distractor_5\",\"distractor_6\",\"distractor_7\",\"distractor_8\")\n",
        "    def test_text_pipeline(context,\n",
        "        ground_truth_utterance,\n",
        "        distractor_0,\n",
        "        distractor_1,\n",
        "        distractor_2,\n",
        "        distractor_3,\n",
        "        distractor_4,\n",
        "        distractor_5,\n",
        "        distractor_6,\n",
        "        distractor_7,\n",
        "        distractor_8):\n",
        "        \"\"\"Processes the transcriptions to generate proper labels\"\"\"\n",
        "        context = torch.LongTensor(preprocess_text(context))\n",
        "        yield context\n",
        "        ground_truth_utterance = torch.LongTensor(preprocess_text(ground_truth_utterance))\n",
        "        yield ground_truth_utterance\n",
        "        distractor_0 = torch.LongTensor(preprocess_text(distractor_0))\n",
        "        yield distractor_0\n",
        "        distractor_1 = torch.LongTensor(preprocess_text(distractor_1))\n",
        "        yield distractor_1\n",
        "        distractor_2 = torch.LongTensor(preprocess_text(distractor_2))\n",
        "        yield distractor_2\n",
        "        distractor_3 = torch.LongTensor(preprocess_text(distractor_3))\n",
        "        yield distractor_3\n",
        "        distractor_4 = torch.LongTensor(preprocess_text(distractor_4))\n",
        "        yield distractor_4\n",
        "        distractor_5 = torch.LongTensor(preprocess_text(distractor_5))\n",
        "        yield distractor_5\n",
        "        distractor_6 = torch.LongTensor(preprocess_text(distractor_6))\n",
        "        yield distractor_6\n",
        "        distractor_7 = torch.LongTensor(preprocess_text(distractor_7))\n",
        "        yield distractor_7\n",
        "        distractor_8 = torch.LongTensor(preprocess_text(distractor_8))\n",
        "        yield distractor_8\n",
        "\n",
        "    # Define datasets from json data manifest file\n",
        "    # Define datasets sorted by ascending lengths for efficiency\n",
        "    datasets = {}\n",
        "    data_info = {\n",
        "        \"train\": hparams[\"train_annotation\"],\n",
        "        \"valid\": hparams[\"valid_annotation\"],\n",
        "        \"test\": hparams[\"test_annotation\"],\n",
        "    }\n",
        "    \n",
        "    # The label encoder will assign a different integer to each element\n",
        "    # in the output vocabulary \n",
        "    # TODO: This needs to be changes\n",
        "    input_encoder = sb.dataio.encoder.CTCTextEncoder()\n",
        "    label_encoder = sb.dataio.encoder.CTCTextEncoder()\n",
        "\n",
        "    for dataset in data_info:\n",
        "        # We will focus on the train dataset, because it's different from the train and val\n",
        "        if dataset == \"train\":\n",
        "            datasets[dataset] = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "                csv_path=data_info[dataset],\n",
        "                dynamic_items=[train_text_pipeline],\n",
        "                output_keys=[\"context\",\"utterance\",\"label\"],\n",
        "            )\n",
        "            hparams[f\"{dataset}_dataloader_opts\"][\"shuffle\"] = True\n",
        "            print(\"done\")\n",
        "        else:\n",
        "            # TODO Later\n",
        "            datasets[dataset] = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "                csv_path=data_info[dataset],\n",
        "                dynamic_items=[test_text_pipeline],\n",
        "                output_keys=[\n",
        "                    \"context\",\n",
        "                    \"ground_truth_utterance\",\n",
        "                    \"distractor_0\",\n",
        "                    \"distractor_1\",\n",
        "                    \"distractor_2\",\n",
        "                    \"distractor_3\",\n",
        "                    \"distractor_4\",\n",
        "                    \"distractor_5\",\n",
        "                    \"distractor_6\",\n",
        "                    \"distractor_7\",\n",
        "                    \"distractor_8\"\n",
        "                ],\n",
        "            )\n",
        "            hparams[f\"{dataset}_dataloader_opts\"][\"shuffle\"] = True\n",
        "            pass\n",
        "\n",
        "    return datasets\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Reading command line arguments\n",
        "    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n",
        "    \n",
        "    # Load hyperparameters file with command-line overrides\n",
        "    with open(hparams_file) as fin:\n",
        "        hparams = load_hyperpyyaml(fin, overrides)\n",
        "\n",
        "    # Create experiment directory\n",
        "    sb.create_experiment_directory(\n",
        "        experiment_directory=hparams[\"output_folder\"],\n",
        "        hyperparams_to_save=hparams_file,\n",
        "        overrides=overrides,\n",
        "    )\n",
        "\n",
        "\n",
        "    # We can now directly create the datasets for training, valid, and test\n",
        "    datasets = dataio_prepare(hparams)\n",
        "\n",
        "    # Trainer initialization\n",
        "    ubuntu_brain = UbuntuBrain(\n",
        "        modules=hparams[\"modules\"],\n",
        "        opt_class=hparams[\"opt_class\"],\n",
        "        hparams=hparams,\n",
        "        run_opts=run_opts,\n",
        "        checkpointer=hparams[\"checkpointer\"],\n",
        "    )\n",
        "    \n",
        "    # Making label encoder accessible (needed to compute the character error rate)\n",
        "    # translate_brain.label_encoder = label_encoder\n",
        "\n",
        "    # The `fit()` method iterates the training loop, calling the methods\n",
        "    # necessary to update the parameters of the model. Since all objects\n",
        "    # with changing state are managed by the Checkpointer, training can be\n",
        "    # stopped at any point, and will be resumed on next call.\n",
        "    ubuntu_brain.fit(\n",
        "        ubuntu_brain.hparams.epoch_counter,\n",
        "        datasets[\"train\"],\n",
        "        datasets[\"valid\"],\n",
        "        train_loader_kwargs=hparams[\"train_dataloader_opts\"],\n",
        "        valid_loader_kwargs=hparams[\"valid_dataloader_opts\"],\n",
        "    )\n",
        "\n",
        "    # Load best checkpoint for evaluation\n",
        "    test_stats = ubuntu_brain.evaluate(\n",
        "        test_set=datasets[\"test\"],\n",
        "        min_key=\"WER\",\n",
        "        test_loader_kwargs=hparams[\"test_dataloader_opts\"],\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQVbTdKLNaiY"
      },
      "source": [
        "**Run the code below** to start training. We here use a CPU because the task is pretty fast. It might take 10-15 minutes to train the RNN model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Suy1R7LA0l9",
        "outputId": "73ffdd1c-1642-4321-b5fe-ae7dc2bda238"
      },
      "outputs": [],
      "source": [
        "!python train.py LSTM_Single_Encoder.yaml --data_folder='./src' --device='cpu' --number_of_epochs=3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLKw91H6EAbN"
      },
      "source": [
        "At test time, we print both hypotheses and references. If everything works well, you should have learned a very plausible translation from Ingnotush to English:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77Mdt3LcEZaM"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "snt136\n",
        "INP: pojiohtta8tthah94poozattzya7ttm6ttaho7ozpooz\n",
        "REF: TERESA WAS SIXTEEN AND VAMPA SEVENTEEN\n",
        "HYP: TERESA WAS SIXTEEN AND VAMPA SEVENTEEN \n",
        "--------\n",
        "snt152\n",
        "INP: 39ha3ottjipattqp95za9ha6jioppfa8ottxnao7ozafop\n",
        "REF: HIS HEART ACTION IS PRETTY WEAK EVEN YET\n",
        "HYP: HIS HEART ACTION IS PRETTY WEAK EVEN YET\n",
        "--------\n",
        "snt103\n",
        "INP: 2ooa2ooap3oa1ji52a3566oyaho7ojittklap9moh\n",
        "REF: GEE GEE THE FROG HOPPED SEVERAL TIMES\n",
        "HYP: GEE GEE THE FROG HOPPED SEVERAL TIMES\n",
        "--------\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woDJc-qbFUUM"
      },
      "source": [
        "Note that some small artifacts in the translation are still possible (e.g., repeating the last character multiple times) because we are using a very simple system. Nevertheless, on average, the quality should be really good. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-0CxiNnEYBH"
      },
      "source": [
        "Your `train_log.txt` file (see `results/RNN/1986/train_log.txt`) should look like this:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "epoch: 1, lr: 1.00e-02 - train loss: 3.58 - valid loss: 2.61\n",
        "epoch: 2, lr: 9.82e-03 - train loss: 2.45 - valid loss: 2.30\n",
        "epoch: 3, lr: 9.63e-03 - train loss: 2.25 - valid loss: 2.15\n",
        "epoch: 4, lr: 9.45e-03 - train loss: 2.11 - valid loss: 2.03\n",
        "epoch: 5, lr: 9.27e-03 - train loss: 1.98 - valid loss: 1.87\n",
        "epoch: 6, lr: 9.08e-03 - train loss: 1.78 - valid loss: 1.54\n",
        "epoch: 7, lr: 8.90e-03 - train loss: 1.32 - valid loss: 7.07e-01\n",
        "epoch: 8, lr: 8.71e-03 - train loss: 6.72e-01 - valid loss: 2.80e-01\n",
        "epoch: 9, lr: 8.53e-03 - train loss: 3.74e-01 - valid loss: 1.54e-01\n",
        "epoch: 10, lr: 8.35e-03 - train loss: 2.61e-01 - valid loss: 1.23e-01\n",
        "epoch: 11, lr: 8.16e-03 - train loss: 2.06e-01 - valid loss: 1.01e-01\n",
        "epoch: 12, lr: 7.98e-03 - train loss: 1.62e-01 - valid loss: 7.95e-02\n",
        "epoch: 13, lr: 7.80e-03 - train loss: 1.51e-01 - valid loss: 7.01e-02\n",
        "epoch: 14, lr: 7.61e-03 - train loss: 1.29e-01 - valid loss: 6.27e-02\n",
        "epoch: 15, lr: 7.43e-03 - train loss: 1.11e-01 - valid loss: 5.58e-02\n",
        "epoch: 16, lr: 7.24e-03 - train loss: 1.04e-01 - valid loss: 4.95e-02\n",
        "epoch: 17, lr: 7.06e-03 - train loss: 9.31e-02 - valid loss: 4.60e-02\n",
        "epoch: 18, lr: 6.88e-03 - train loss: 7.69e-02 - valid loss: 4.27e-02\n",
        "epoch: 19, lr: 6.69e-03 - train loss: 7.32e-02 - valid loss: 4.72e-02\n",
        "epoch: 20, lr: 6.51e-03 - train loss: 7.17e-02 - valid loss: 4.22e-02\n",
        "epoch: 21, lr: 6.33e-03 - train loss: 6.75e-02 - valid loss: 3.27e-02\n",
        "epoch: 22, lr: 6.14e-03 - train loss: 6.18e-02 - valid loss: 3.40e-02\n",
        "epoch: 23, lr: 5.96e-03 - train loss: 5.32e-02 - valid loss: 2.78e-02\n",
        "epoch: 24, lr: 5.78e-03 - train loss: 4.97e-02 - valid loss: 2.99e-02\n",
        "epoch: 25, lr: 5.59e-03 - train loss: 4.88e-02 - valid loss: 2.89e-02\n",
        "epoch: 26, lr: 5.41e-03 - train loss: 5.18e-02 - valid loss: 2.99e-02\n",
        "epoch: 27, lr: 5.22e-03 - train loss: 4.13e-02 - valid loss: 2.57e-02\n",
        "epoch: 28, lr: 5.04e-03 - train loss: 3.57e-02 - valid loss: 2.55e-02\n",
        "epoch: 29, lr: 4.86e-03 - train loss: 3.45e-02 - valid loss: 2.48e-02\n",
        "epoch: 30, lr: 4.67e-03 - train loss: 3.40e-02 - valid loss: 2.41e-02\n",
        "epoch: 31, lr: 4.49e-03 - train loss: 2.88e-02 - valid loss: 2.13e-02\n",
        "epoch: 32, lr: 4.31e-03 - train loss: 3.01e-02 - valid loss: 2.03e-02\n",
        "epoch: 33, lr: 4.12e-03 - train loss: 3.11e-02 - valid loss: 2.36e-02\n",
        "epoch: 34, lr: 3.94e-03 - train loss: 2.63e-02 - valid loss: 2.00e-02\n",
        "epoch: 35, lr: 3.76e-03 - train loss: 2.51e-02 - valid loss: 2.31e-02\n",
        "epoch: 36, lr: 3.57e-03 - train loss: 2.32e-02 - valid loss: 2.09e-02\n",
        "epoch: 37, lr: 3.39e-03 - train loss: 2.09e-02 - valid loss: 1.98e-02\n",
        "epoch: 38, lr: 3.20e-03 - train loss: 1.98e-02 - valid loss: 1.86e-02\n",
        "epoch: 39, lr: 3.02e-03 - train loss: 1.94e-02 - valid loss: 1.81e-02\n",
        "epoch: 40, lr: 2.84e-03 - train loss: 1.82e-02 - valid loss: 2.12e-02\n",
        "epoch: 41, lr: 2.65e-03 - train loss: 1.63e-02 - valid loss: 1.88e-02\n",
        "epoch: 42, lr: 2.47e-03 - train loss: 1.61e-02 - valid loss: 1.79e-02\n",
        "epoch: 43, lr: 2.29e-03 - train loss: 1.51e-02 - valid loss: 1.49e-02\n",
        "epoch: 44, lr: 2.10e-03 - train loss: 1.41e-02 - valid loss: 1.71e-02\n",
        "epoch: 45, lr: 1.92e-03 - train loss: 1.42e-02 - valid loss: 1.71e-02\n",
        "epoch: 46, lr: 1.73e-03 - train loss: 1.32e-02 - valid loss: 1.60e-02\n",
        "epoch: 47, lr: 1.55e-03 - train loss: 1.24e-02 - valid loss: 1.56e-02\n",
        "epoch: 48, lr: 1.37e-03 - train loss: 1.50e-02 - valid loss: 1.56e-02\n",
        "epoch: 49, lr: 1.18e-03 - train loss: 1.23e-02 - valid loss: 1.49e-02\n",
        "epoch: 50, lr: 1.00e-03 - train loss: 1.19e-02 - valid loss: 1.46e-02\n",
        "Epoch loaded: 50 - test loss: 2.90e-02\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6qRgb7TF-nE"
      },
      "source": [
        "Note that variations to the performance are expected. The most important thing is to get a good test loss (e.g., < 4.00e-02).\n",
        "\n",
        "We can now plot the training and validation losses stored in the log file.\n",
        "\n",
        "**Complete the code below** to plot the training and validation curves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "99fVncKpF909",
        "outputId": "35b9d6bf-5751-4673-d3ff-9678d10492fc"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def get_losses(log_file):\n",
        "  \"\"\"This function takes in input a path of a log-file and outputs the train and\n",
        "  valid losses in lists of float numbers\"\"\"\n",
        "\n",
        "  # Your code here. Aim for 9-10 lines\n",
        "  with open(log_file) as f:\n",
        "    lines = f.readlines()\n",
        "  train_losses = []\n",
        "  valid_losses = []\n",
        "  for l in lines[:-1]:\n",
        "    vals = l.replace(',','').split()\n",
        "    train_losses.append(float(vals[7]))\n",
        "    valid_losses.append(float(vals[-1]))\n",
        "    \n",
        "  return train_losses, valid_losses\n",
        "\n",
        "log_file = 'results/RNN/1986/train_log.txt'\n",
        "train_losses, valid_losses = get_losses(log_file)\n",
        "\n",
        "plt.plot(train_losses, label='train')\n",
        "plt.plot(valid_losses, label='valid')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('# Epochs')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHp3xFnfKE39"
      },
      "source": [
        "You should get something like this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjMHzb6aKLXS"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "jupyter",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "a188046c38a3d988ebac87985afd6adffffc9be3924e19d2e5b9257a2ccc7129"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
