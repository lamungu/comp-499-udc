{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "These recent advancements have proven to be beneficial to the field of conversational AI and Natural Language Processing (NLP). With the latest release of GPT-3 and advent of GPT-4, as well as the prominence of Large Language Models (LLMs) such as Microsoft T5 and Google's BERT, many tasks that have considered to be majorly solved through conventional approaches have been able to breach levels of performance previously unattainable. \n",
    "\n",
    "The question remains as to the ability to make use of these models in the every day world, and how it can pierce into the industry. Amongst these place would be the use of intelligent dialogue systems within service departments, where the understanding of context is critical to providing the right aid to customers often requiring time-sensitive intervention from experts. \n",
    "\n",
    "In this project, we will be exploring the improvements pre-trained models can bring by matching them next to ones that have been done in the past. We will be using the Ubuntu Dialogue Corpus, our goal will be to assess how well a model can provide context-relevant responses in an end-to-end dialogue system. these approaches while exploring the Ubuntu Dialogue Corpus [Citation] and training an End-to-End dialogue system."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "## Ubuntu Dialogue Corpus\n",
    "\n",
    "The Ubuntu Dialogue Dataset was created from a compilation of over 1.3 million dialogues out of an IRC chat between users and support agents. The conversations are considered dyadic, meaning they typically happen between two people. As well, there are other things that are important like the fact that the chats are mixed in together. \n",
    "\n",
    "[Insert a picture of how it should look and stuff]\n",
    "\n",
    "The process to create the dataset was to primarily untangle the conversation such that one set of utterances deal with one topic\n",
    "\n",
    "The training dataset has three fields: context, representing the preceding conversation utterance; response, being the target utterance to the conversation; and label, which determines whether the response is contextual to the previous utterances. For our set task, the non-contextual samples (with a label value of 0) are sampled out from the other utterances in the dataset. \n",
    "\n",
    "As for the validation and test dataset, 11 columns are provided: the context, similar to the training set's; the ground truth utterance, which correspond to the actual utterance that written by the speaker; and nine distractors, corresponding to different utterances sampled out of other conversations. For our set task, only the ground truth utterance will be kept, as we will attempt at getting a most similar response to that of the human conversationalists.\n",
    "\n",
    "## Preprocessing\n",
    "\n",
    "In order to make use of the text, some preprocessing techniques were used to ensure the best features are extracted by our end to end system.\n",
    "\n",
    "### Data Cleaning\n",
    "\n",
    "Conversations made from chat rooms such as those found in the UDC can often be riddled with special characters to denote emotions or flavor to a text. While it may be interesting to determine the importance of these inclusions, we decided to omit these in order to limit to actual sentence constructions out of the tokens used. \n",
    "\n",
    "### Path replacements\n",
    "\n",
    "Due to the nature of the Ubuntu Support community, discussions invite the heavy use of different Internet URLs and directory paths. We opted to generalize them under control tokens (`__url__` and `__path__`), reducing the amount of vocabulary that would have been generated the vast majority of unique tokens. The same approach is carried over for cardinal numbers. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "\n",
    "Natural Language Processing revolves a lot on the features extracted from the words used. Unlike sound and images, where numerical values can be extracted out of frequencies and color values, word embeddings have to infer relations between them in order to create a linearly separable space. This can be quite subjective, and as such, many approaches exist to ensure the tokenization process leads to features that can correctly represent the data.\n",
    "\n",
    "# Word2vec\n",
    "\n",
    "# GLoVe\n",
    "\n",
    "# SentencePiece\n",
    "\n",
    "Limits the vocabulary to 32000. This would give it a great advantage at being a very small dictionary with a big amount of coverage due to its documentation of subwords.\n",
    "Fea\n",
    "# Approaches\n",
    "\n",
    "\n",
    "## Encoder-Decoder RNN with LSTM \n",
    "\n",
    "As a baseline, we've set up an encoder-decoder recurrent neural network (RNN) with long short-term memory (LSTM) cells. The use of the LSTM cells helps in retaining long-term dependencies, which can benefit greatly to acquiring context of a discussion. As an input, the Encoder receives only the last utterance from the discussion, with\n",
    "\n",
    "## Hierarchical Recurrent Encoder-Decoder (HRED)\n",
    "\n",
    "\n",
    "## GPT-2 Transformer\n",
    "\n",
    "As a final approach is to replace the encoder-decoder for a pretrained transformer using OpenAI's Generative Pre-trained Transformer (GPT). (Give an explanation as to why we chose GPT-2) \n",
    "\n",
    "Unlike the Encoder-Decoder, GPT-2 only takes in one input to finetune for the downstream task. We've adjusted the input in order to accomodate for it.\n",
    "\n",
    "Previous input: Context -> Response\n",
    "New input: <|context|>Context<|response|>Response"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
